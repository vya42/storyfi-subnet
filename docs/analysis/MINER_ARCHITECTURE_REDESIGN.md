# StoryFi Miner Architecture Redesign - æ­£ç¡®çš„å»ä¸­å¿ƒåŒ–å®ç°

## ğŸš¨ å½“å‰é—®é¢˜

### é”™è¯¯çš„è®¾è®¡ (neurons/miner_gemini.py)
```python
# âŒ æ‰€æœ‰çŸ¿å·¥å¼ºåˆ¶ä½¿ç”¨ Gemini API
self.model = genai.GenerativeModel("gemini-2.5-flash")

# é—®é¢˜:
1. ä¸æ˜¯çœŸæ­£çš„å»ä¸­å¿ƒåŒ–ï¼ˆä¾èµ–Googleå•ç‚¹ï¼‰
2. æ‰€æœ‰çŸ¿å·¥ç”¨åŒä¸€ä¸ªæ¨¡å‹â†’å“åº”é«˜åº¦ç›¸ä¼¼
3. çŸ¿å·¥å¿…é¡»ä»˜è´¹ç»™Google
4. Googleæ•…éšœâ†’æ‰€æœ‰çŸ¿å·¥å¤±è´¥
5. ä¸ç¬¦åˆBittensorè®¾è®¡ç†å¿µ
```

---

## âœ… æ­£ç¡®çš„è®¾è®¡ - å‚è€ƒTop 5å­ç½‘

### ç ”ç©¶æˆæœæ€»ç»“

| å­ç½‘ | çŸ¿å·¥æ–¹å¼ | ç¡¬ä»¶è¦æ±‚ | å»ä¸­å¿ƒåŒ–ç¨‹åº¦ |
|------|---------|---------|-------------|
| SN1 (Text Prompting) | âŒ ä½¿ç”¨OpenAI API | 1 vCPU | â­ ä½ï¼ˆä¾èµ–OpenAIï¼‰ |
| SN19 (Vision) | âœ… æœ¬åœ°GPUè¿è¡Œå¼€æºæ¨¡å‹ | A100 80GB | â­â­â­â­â­ é«˜ |
| SN27 (Compute) | âœ… æä¾›GPUç®—åŠ› | H100/A100 | â­â­â­â­â­ é«˜ |
| SN64 (Chutes) | âœ… æœ¬åœ°æ¨ç† | A100+ | â­â­â­â­â­ é«˜ |
| SN34 (BitMind) | âœ… æœ¬åœ°è®­ç»ƒ/æ¨ç† | Consumer GPU+ | â­â­â­â­ é«˜ |

**ç»“è®º**: æˆåŠŸçš„å­ç½‘éƒ½ä½¿ç”¨**æœ¬åœ°GPUè¿è¡Œå¼€æºæ¨¡å‹**ï¼Œè€Œéä¾èµ–APIã€‚

---

## ğŸ¯ æ–°æ¶æ„è®¾è®¡

### Phase 2.0: æ··åˆæ¶æ„ï¼ˆæœ¬åœ°ä¼˜å…ˆ + APIå¤‡ä»½ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         StoryFi Miner v2.0              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Model Loader (æ™ºèƒ½é€‰æ‹©)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â”‚                          â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚      â”‚               â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Local  â”‚    â”‚   API    â”‚           â”‚
â”‚  â”‚ GPU    â”‚    â”‚ Fallback â”‚           â”‚
â”‚  â”‚ Models â”‚    â”‚ (Gemini) â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚  ä¼˜å…ˆçº§1        ä¼˜å…ˆçº§2                â”‚
â”‚  å¥–åŠ± 1.0x      å¥–åŠ± 0.5x              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒåŸåˆ™

1. **æœ¬åœ°ä¼˜å…ˆ**: ä¼˜å…ˆä½¿ç”¨çŸ¿å·¥è‡ªå·±çš„GPUå’Œæ¨¡å‹
2. **æ¨¡å‹å¤šæ ·æ€§**: æ”¯æŒå¤šç§å¼€æºæ¨¡å‹ï¼ˆLlama, Mixtral, Qwenç­‰ï¼‰
3. **APIå¤‡ä»½**: æ²¡æœ‰GPUçš„çŸ¿å·¥å¯ä»¥é™çº§ä½¿ç”¨APIï¼ˆä½†å¥–åŠ±å‡åŠï¼‰
4. **å…¬å¹³å¥–åŠ±**: æ ¹æ®ç¡¬ä»¶æŠ•å…¥å’Œå»ä¸­å¿ƒåŒ–è´¡çŒ®è°ƒæ•´å¥–åŠ±

---

## ğŸ“¦ æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨

### Tier 1: é«˜æ€§èƒ½æ¨¡å‹ï¼ˆ80GB+ VRAMï¼‰

```yaml
llama-3-70b-instruct:
  repo: "meta-llama/Meta-Llama-3-70B-Instruct"
  vram: 70GB
  reward_multiplier: 1.5x
  quality: æœ€é«˜

mixtral-8x7b-instruct:
  repo: "mistralai/Mixtral-8x7B-Instruct-v0.1"
  vram: 45GB
  reward_multiplier: 1.3x
  quality: å¾ˆé«˜

qwen-72b-chat:
  repo: "Qwen/Qwen-72B-Chat"
  vram: 72GB
  reward_multiplier: 1.4x
  quality: å¾ˆé«˜
```

### Tier 2: ä¸­ç­‰æ¨¡å‹ï¼ˆ24-40GB VRAMï¼‰

```yaml
llama-3-8b-instruct:
  repo: "meta-llama/Meta-Llama-3-8B-Instruct"
  vram: 16GB
  reward_multiplier: 1.0x
  quality: è‰¯å¥½

mistral-7b-instruct:
  repo: "mistralai/Mistral-7B-Instruct-v0.2"
  vram: 14GB
  reward_multiplier: 1.0x
  quality: è‰¯å¥½

yi-34b-chat:
  repo: "01-ai/Yi-34B-Chat"
  vram: 34GB
  reward_multiplier: 1.2x
  quality: å¾ˆå¥½
```

### Tier 3: è½»é‡æ¨¡å‹ï¼ˆ<16GB VRAMï¼‰

```yaml
phi-3-medium:
  repo: "microsoft/Phi-3-medium-4k-instruct"
  vram: 8GB
  reward_multiplier: 0.8x
  quality: ä¸­ç­‰

gemma-7b-it:
  repo: "google/gemma-7b-it"
  vram: 14GB
  reward_multiplier: 0.9x
  quality: è‰¯å¥½
```

### Tier 4: APIå¤‡ä»½ï¼ˆæ— GPUï¼‰

```yaml
gemini-2.5-flash:
  provider: Google
  cost: æŒ‰è°ƒç”¨ä»˜è´¹
  reward_multiplier: 0.5x
  quality: å¾ˆé«˜
  note: ä»…ä½œå¤‡ä»½ï¼Œä¸é¼“åŠ±ä½¿ç”¨

gpt-4-turbo:
  provider: OpenAI
  cost: æŒ‰è°ƒç”¨ä»˜è´¹
  reward_multiplier: 0.5x
  quality: æœ€é«˜
  note: ä»…ä½œå¤‡ä»½ï¼Œä¸é¼“åŠ±ä½¿ç”¨
```

---

## ğŸ’» æ–°Minerå®ç°

### neurons/miner_v2.py

```python
"""
StoryFi Miner v2.0 - æœ¬åœ°GPUä¼˜å…ˆæ¶æ„
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Optional, Dict, Any
import bittensor as bt
import os

class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    SUPPORTED_MODELS = {
        # Tier 1: é«˜æ€§èƒ½æ¨¡å‹
        "llama-3-70b": {
            "repo": "meta-llama/Meta-Llama-3-70B-Instruct",
            "vram_gb": 70,
            "reward_multiplier": 1.5,
            "load_in_8bit": False,
            "load_in_4bit": True  # ä½¿ç”¨4bité‡åŒ–é™ä½VRAMéœ€æ±‚
        },
        "mixtral-8x7b": {
            "repo": "mistralai/Mixtral-8x7B-Instruct-v0.1",
            "vram_gb": 45,
            "reward_multiplier": 1.3,
            "load_in_8bit": True,
            "load_in_4bit": False
        },

        # Tier 2: ä¸­ç­‰æ¨¡å‹
        "llama-3-8b": {
            "repo": "meta-llama/Meta-Llama-3-8B-Instruct",
            "vram_gb": 16,
            "reward_multiplier": 1.0,
            "load_in_8bit": True,
            "load_in_4bit": False
        },
        "mistral-7b": {
            "repo": "mistralai/Mistral-7B-Instruct-v0.2",
            "vram_gb": 14,
            "reward_multiplier": 1.0,
            "load_in_8bit": True,
            "load_in_4bit": False
        },

        # Tier 3: è½»é‡æ¨¡å‹
        "phi-3-medium": {
            "repo": "microsoft/Phi-3-medium-4k-instruct",
            "vram_gb": 8,
            "reward_multiplier": 0.8,
            "load_in_8bit": True,
            "load_in_4bit": False
        }
    }


class LocalModelLoader:
    """æœ¬åœ°æ¨¡å‹åŠ è½½å™¨"""

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.config = ModelConfig.SUPPORTED_MODELS.get(model_name)

        if not self.config:
            raise ValueError(f"Unsupported model: {model_name}")

        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def load(self):
        """åŠ è½½æ¨¡å‹åˆ°GPU"""
        bt.logging.info(f"Loading model: {self.config['repo']}")
        bt.logging.info(f"Target device: {self.device}")
        bt.logging.info(f"Required VRAM: {self.config['vram_gb']}GB")

        # æ£€æŸ¥GPUå†…å­˜
        if self.device == "cuda":
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            bt.logging.info(f"Available GPU memory: {gpu_memory:.1f}GB")

            if gpu_memory < self.config['vram_gb'] * 0.8:
                bt.logging.warning(
                    f"GPU memory may be insufficient. "
                    f"Required: {self.config['vram_gb']}GB, "
                    f"Available: {gpu_memory:.1f}GB"
                )

        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config['repo'],
            trust_remote_code=True
        )

        # åŠ è½½æ¨¡å‹ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©é‡åŒ–æ–¹å¼ï¼‰
        load_kwargs = {
            "pretrained_model_name_or_path": self.config['repo'],
            "device_map": "auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¤šGPU
            "torch_dtype": torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
            "trust_remote_code": True
        }

        # é‡åŒ–é€‰é¡¹
        if self.config.get('load_in_4bit'):
            from transformers import BitsAndBytesConfig
            load_kwargs["quantization_config"] = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True
            )
            bt.logging.info("Using 4-bit quantization")

        elif self.config.get('load_in_8bit'):
            load_kwargs["load_in_8bit"] = True
            bt.logging.info("Using 8-bit quantization")

        self.model = AutoModelForCausalLM.from_pretrained(**load_kwargs)

        bt.logging.success(f"âœ… Model loaded successfully")
        return self

    def generate(self, prompt: str, max_new_tokens: int = 2000) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if self.model is None:
            raise RuntimeError("Model not loaded. Call load() first.")

        # Tokenizeè¾“å…¥
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        generated_text = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )

        # ç§»é™¤åŸå§‹prompt
        response = generated_text[len(prompt):].strip()
        return response


class APIFallbackLoader:
    """APIå¤‡ä»½åŠ è½½å™¨ï¼ˆç”¨äºæ²¡æœ‰GPUçš„çŸ¿å·¥ï¼‰"""

    def __init__(self, provider: str = "gemini"):
        self.provider = provider

        if provider == "gemini":
            import google.generativeai as genai
            genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
            self.model = genai.GenerativeModel("gemini-2.5-flash")

        elif provider == "openai":
            import openai
            openai.api_key = os.getenv("OPENAI_API_KEY")
            self.model = openai.ChatCompletion

        else:
            raise ValueError(f"Unsupported API provider: {provider}")

        bt.logging.warning("âš ï¸  Using API fallback mode (0.5x rewards)")

    def generate(self, prompt: str) -> str:
        """é€šè¿‡APIç”Ÿæˆæ–‡æœ¬"""
        if self.provider == "gemini":
            response = self.model.generate_content(prompt)
            return response.text

        elif self.provider == "openai":
            response = self.model.create(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content


class StoryFiMinerV2:
    """StoryFi Miner v2.0 - æ™ºèƒ½æ¨¡å‹é€‰æ‹©"""

    def __init__(self, config=None):
        self.config = config or get_config()

        # åˆå§‹åŒ–Bittensorç»„ä»¶
        self.wallet = bt.wallet(config=self.config)
        self.subtensor = bt.subtensor(config=self.config)
        self.metagraph = self.subtensor.metagraph(self.config.netuid)

        # æ™ºèƒ½é€‰æ‹©æ¨¡å‹
        self.model_loader = self.initialize_model()
        self.mode = self.model_loader.__class__.__name__

        # è®¾ç½®Axon
        self.axon = bt.axon(wallet=self.wallet, config=self.config)

        bt.logging.info(f"âœ… Miner initialized in {self.mode} mode")

    def initialize_model(self):
        """æ™ºèƒ½é€‰æ‹©æœ€ä½³æ¨¡å‹"""

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰GPU
        if not torch.cuda.is_available():
            bt.logging.warning("No CUDA GPU detected. Falling back to API mode.")
            return APIFallbackLoader(provider="gemini")

        # 2. æ£€æŸ¥GPUå†…å­˜
        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        bt.logging.info(f"Detected GPU memory: {gpu_memory_gb:.1f}GB")

        # 3. ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹é€‰æ‹©
        preferred_model = self.config.model_name or os.getenv("MODEL_NAME", "auto")

        if preferred_model == "auto":
            # è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
            if gpu_memory_gb >= 70:
                selected_model = "llama-3-70b"
            elif gpu_memory_gb >= 45:
                selected_model = "mixtral-8x7b"
            elif gpu_memory_gb >= 16:
                selected_model = "llama-3-8b"
            elif gpu_memory_gb >= 8:
                selected_model = "phi-3-medium"
            else:
                bt.logging.warning(
                    f"GPU memory ({gpu_memory_gb:.1f}GB) insufficient for any local model. "
                    "Falling back to API mode."
                )
                return APIFallbackLoader(provider="gemini")
        else:
            selected_model = preferred_model

        # 4. åŠ è½½æœ¬åœ°æ¨¡å‹
        try:
            bt.logging.info(f"Attempting to load local model: {selected_model}")
            loader = LocalModelLoader(selected_model)
            loader.load()

            # æŠ¥å‘Šå¥–åŠ±å€æ•°
            reward_multiplier = loader.config['reward_multiplier']
            bt.logging.success(
                f"âœ… Local model loaded successfully. "
                f"Reward multiplier: {reward_multiplier}x"
            )

            return loader

        except Exception as e:
            bt.logging.error(f"Failed to load local model: {e}")
            bt.logging.warning("Falling back to API mode")
            return APIFallbackLoader(provider="gemini")

    async def generate_blueprint(self, synapse):
        """ç”Ÿæˆæ•…äº‹è“å›¾"""
        start_time = time.time()

        prompt = self.build_blueprint_prompt(synapse.user_input)

        # ä½¿ç”¨åŠ è½½çš„æ¨¡å‹ç”Ÿæˆ
        response_text = self.model_loader.generate(prompt)

        # è§£æJSON
        try:
            output_data = json.loads(response_text)
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯çº¯JSONï¼Œå°è¯•æå–
            import re
            json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if json_match:
                output_data = json.loads(json_match.group())
            else:
                bt.logging.error("Failed to parse JSON from model output")
                output_data = {"error": "Invalid JSON output"}

        synapse.output_data = output_data
        synapse.generation_time = time.time() - start_time
        synapse.miner_version = "2.0.0"
        synapse.miner_mode = self.mode  # æ·»åŠ æ¨¡å¼ä¿¡æ¯

        return synapse

    def build_blueprint_prompt(self, user_input: str) -> str:
        """æ„å»ºBlueprintç”Ÿæˆprompt"""
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•…äº‹åˆ›ä½œè€…ã€‚åŸºäºç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ•…äº‹è“å›¾ã€‚

ç”¨æˆ·è¾“å…¥: {user_input}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å­—æ®µçš„JSONæ ¼å¼å“åº”:
{{
  "title": "å¸å¼•äººçš„æ•…äº‹æ ‡é¢˜",
  "genre": "æ•…äº‹ç±»å‹ï¼ˆç§‘å¹»/å¥‡å¹»/æ‚¬ç–‘ç­‰ï¼‰",
  "setting": "è¯¦ç»†çš„èƒŒæ™¯è®¾å®šå’Œä¸–ç•Œè§‚ï¼ˆè‡³å°‘300å­—ï¼‰",
  "core_conflict": "æ ¸å¿ƒå†²çªå’Œä¸»è¦çŸ›ç›¾ï¼ˆè‡³å°‘150å­—ï¼‰",
  "themes": ["ä¸»é¢˜1", "ä¸»é¢˜2", "ä¸»é¢˜3"],
  "tone": "å™äº‹åŸºè°ƒï¼ˆä¸¥è‚ƒ/è½»æ¾/é»‘æš—ç­‰ï¼‰",
  "target_audience": "ç›®æ ‡å—ä¼—"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    # ... å…¶ä»–ç”Ÿæˆæ–¹æ³•ç±»ä¼¼ ...


# é…ç½®è§£æ
def get_config():
    parser = argparse.ArgumentParser()

    # Bittensorå‚æ•°
    parser.add_argument("--netuid", type=int, default=108)
    parser.add_argument("--subtensor.network", type=str, default="test")
    parser.add_argument("--wallet.name", type=str, default="miner")
    parser.add_argument("--wallet.hotkey", type=str, default="default")
    parser.add_argument("--axon.port", type=int, default=8091)

    # æ¨¡å‹é€‰æ‹©å‚æ•°
    parser.add_argument(
        "--model.name",
        type=str,
        default="auto",
        choices=["auto", "llama-3-70b", "llama-3-8b", "mixtral-8x7b",
                 "mistral-7b", "phi-3-medium"],
        help="Model to use. 'auto' will select based on available GPU memory."
    )

    parser.add_argument(
        "--model.fallback_api",
        type=str,
        default="gemini",
        choices=["gemini", "openai"],
        help="API provider to use if local model loading fails"
    )

    return bt.config(parser)


if __name__ == "__main__":
    miner = StoryFiMinerV2()
    asyncio.run(miner.run())
```

---

## ğŸ¯ Validatorè°ƒæ•´ - å¥–åŠ±æœºåˆ¶

### neurons/validator_v2.py ç‰‡æ®µ

```python
def calculate_reward_multiplier(self, miner_info: Dict) -> float:
    """
    æ ¹æ®çŸ¿å·¥æ¨¡å¼è°ƒæ•´å¥–åŠ±

    Args:
        miner_info: {
            "mode": "LocalModelLoader" | "APIFallbackLoader",
            "model_name": "llama-3-70b" | "gemini" | ...,
            "gpu_memory_gb": 80 | None
        }

    Returns:
        å¥–åŠ±å€æ•° (0.5 - 1.5x)
    """

    # 1. APIæ¨¡å¼ï¼šä½å¥–åŠ±
    if miner_info["mode"] == "APIFallbackLoader":
        return 0.5

    # 2. æœ¬åœ°æ¨¡å‹ï¼šæ ¹æ®æ¨¡å‹å¤§å°
    model_name = miner_info.get("model_name", "unknown")

    # ä»ModelConfigè·å–åŸºç¡€å€æ•°
    base_multiplier = ModelConfig.SUPPORTED_MODELS.get(
        model_name, {}
    ).get("reward_multiplier", 1.0)

    # 3. GPUæ€§èƒ½åŠ æˆï¼ˆå¯é€‰ï¼‰
    gpu_memory = miner_info.get("gpu_memory_gb", 0)
    if gpu_memory >= 80:
        base_multiplier *= 1.1  # H100/A100 10%åŠ æˆ

    return base_multiplier


def score_response_with_multiplier(self, response, task_type: str) -> float:
    """
    è¯„åˆ† + å¥–åŠ±å€æ•°
    """
    # 1. åŸºç¡€è¯„åˆ† (0-100)
    base_score = self.score_response(response, task_type)

    # 2. æå–çŸ¿å·¥ä¿¡æ¯
    miner_info = {
        "mode": response.miner_mode,
        "model_name": getattr(response, "model_name", None),
        "gpu_memory_gb": getattr(response, "gpu_memory_gb", None)
    }

    # 3. è®¡ç®—å¥–åŠ±å€æ•°
    multiplier = self.calculate_reward_multiplier(miner_info)

    # 4. æœ€ç»ˆåˆ†æ•°
    final_score = base_score * multiplier

    bt.logging.info(
        f"Miner {response.miner_hotkey[:8]}: "
        f"Base={base_score:.1f}, "
        f"Multiplier={multiplier:.2f}x, "
        f"Final={final_score:.1f}"
    )

    return final_score
```

---

## ğŸ“‹ éƒ¨ç½²æŒ‡å—

### å¯¹äºæœ‰GPUçš„çŸ¿å·¥

```bash
# 1. å®‰è£…ä¾èµ–
pip install transformers accelerate bitsandbytes torch

# 2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼Œç”¨äºæ‰‹åŠ¨é€‰æ‹©æ¨¡å‹ï¼‰
export MODEL_NAME="llama-3-8b"  # æˆ– "auto"

# 3. è¿è¡ŒçŸ¿å·¥
python neurons/miner_v2.py \
    --netuid 108 \
    --subtensor.network test \
    --wallet.name my_miner \
    --wallet.hotkey default \
    --model.name auto \
    --axon.port 8091
```

### å¯¹äºæ— GPUçš„çŸ¿å·¥ï¼ˆAPIæ¨¡å¼ï¼‰

```bash
# 1. è®¾ç½®API Key
export GEMINI_API_KEY=your_key_here

# 2. è¿è¡ŒçŸ¿å·¥ï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹æ— GPUå¹¶åˆ‡æ¢åˆ°APIæ¨¡å¼ï¼‰
python neurons/miner_v2.py \
    --netuid 108 \
    --subtensor.network test \
    --wallet.name my_miner \
    --wallet.hotkey default \
    --model.fallback_api gemini \
    --axon.port 8091

# æ³¨æ„ï¼šAPIæ¨¡å¼å¥–åŠ±ä»…ä¸ºæœ¬åœ°æ¨¡å¼çš„50%
```

---

## ğŸ”„ è¿ç§»è®¡åˆ’

### Phase 2.1: å®ç°æ–°Miner (1å‘¨)
- [x] ç ”ç©¶Top 5å­ç½‘å®ç°
- [ ] åˆ›å»º `neurons/miner_v2.py`
- [ ] å®ç° `LocalModelLoader`
- [ ] å®ç° `APIFallbackLoader`
- [ ] æ·»åŠ è‡ªåŠ¨æ¨¡å‹é€‰æ‹©é€»è¾‘

### Phase 2.2: æ›´æ–°Validator (3å¤©)
- [ ] ä¿®æ”¹ `neurons/validator.py`
- [ ] æ·»åŠ å¥–åŠ±å€æ•°è®¡ç®—
- [ ] æ›´æ–°è¯„åˆ†ç³»ç»Ÿ

### Phase 2.3: æµ‹è¯• (1å‘¨)
- [ ] æœ¬åœ°æµ‹è¯•ï¼ˆä¸åŒGPUé…ç½®ï¼‰
- [ ] æµ‹è¯•ç½‘éƒ¨ç½²
- [ ] 24å°æ—¶ç¨³å®šæ€§æµ‹è¯•
- [ ] å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è´¨é‡å’Œå¥–åŠ±

### Phase 2.4: æ–‡æ¡£å’Œéƒ¨ç½² (3å¤©)
- [ ] æ›´æ–°éƒ¨ç½²æ–‡æ¡£
- [ ] åˆ›å»ºæ¨¡å‹é€‰æ‹©æŒ‡å—
- [ ] ä¸»ç½‘éƒ¨ç½²

---

## ğŸ“ å­¦åˆ°çš„ç»éªŒ

1. **ä¸è¦é—­é—¨é€ è½¦**: ç ”ç©¶æˆåŠŸæ¡ˆä¾‹æ¯”è‡ªå·±æ‘¸ç´¢æ›´é«˜æ•ˆ
2. **å»ä¸­å¿ƒåŒ–æ˜¯æ ¸å¿ƒ**: Bittensorçš„æœ¬è´¨æ˜¯å»ä¸­å¿ƒåŒ–ï¼Œä¾èµ–å•ä¸€APIè¿èƒŒè®¾è®¡ç†å¿µ
3. **å…¬å¹³æ¿€åŠ±**: å¥–åŠ±åº”è¯¥ä¸çŸ¿å·¥çš„å®é™…ç¡¬ä»¶æŠ•å…¥æˆæ­£æ¯”
4. **æ¨¡å‹å¤šæ ·æ€§**: å…è®¸çŸ¿å·¥é€‰æ‹©ä¸åŒæ¨¡å‹â†’å“åº”å¤šæ ·åŒ–â†’æ›´éš¾ä½œå¼Š
5. **æ¸è¿›å¼è®¾è®¡**: å…ˆæ”¯æŒæœ¬åœ°+APIæ··åˆï¼Œæœªæ¥å¯ä»¥å®Œå…¨ç§»é™¤API

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **SN19 Vision**: https://github.com/rayonlabs/vision-workers
- **SN27 Compute**: https://github.com/neuralinternet/compute-subnet
- **SN1 Text Prompting**: https://github.com/opentensor/text-prompting
- **Transformersåº“**: https://huggingface.co/docs/transformers
- **BitsAndBytesé‡åŒ–**: https://github.com/TimDettmers/bitsandbytes

---

**ç‰ˆæœ¬**: 2.0.0
**ä½œè€…**: StoryFi Team
**æ—¥æœŸ**: 2025-10-23
**çŠ¶æ€**: è®¾è®¡å®Œæˆï¼Œå¾…å®ç°

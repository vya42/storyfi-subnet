#!/usr/bin/env python3
"""
Test script for Narrative Merit Scoring Module.

This script tests the AI-based narrative evaluation without requiring
a full Bittensor setup.

Usage:
    python tests/test_narrative_scoring.py
"""

import sys
import os
import json

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scoring.narrative import (
    calculate_narrative_score,
    NarrativeEvaluator,
    get_evaluator
)


def test_blueprint_scoring():
    """Test scoring a blueprint task."""
    print("\n" + "=" * 60)
    print("TEST 1: Blueprint Scoring")
    print("=" * 60)

    data = {
        "title": "ÊòüÈôÖËø∑ÈÄî",
        "genre": "ÁßëÂπªÂÜíÈô©",
        "setting": "2150Âπ¥Ôºå‰∫∫Á±ªÂ∑≤ÁªèÂª∫Á´ã‰∫ÜÊ®™Ë∑®‰∏â‰∏™ÊòüÁ≥ªÁöÑÊÆñÊ∞ëÂ∏ùÂõΩ„ÄÇ‰∏ªËßíÁîüÊ¥ªÂú®ËæπÁºòÊòüÁêÉÔºåÈÇ£ÈáåËµÑÊ∫êÂåÆ‰πè‰ΩÜËá™Áî±Â∫¶È´ò„ÄÇ",
        "core_conflict": "‰∏ÄËâòÁ•ûÁßòÁöÑÂ§ñÊòüÈ£ûËàπÂù†ËêΩÂú®‰∏ªËßíÁöÑÊòüÁêÉ‰∏äÔºåÂ∏¶Êù•‰∫ÜÂèØËÉΩÊîπÂèò‰∫∫Á±ªÂëΩËøêÁöÑÊäÄÊúØÔºå‰ΩÜ‰πüÂºïÊù•‰∫ÜÂ∏ùÂõΩÂÜõÈòüÁöÑÊ≥®ÊÑè„ÄÇ",
        "themes": ["Êé¢Á¥¢Êú™Áü•", "Ëá™Áî±‰∏éÁß©Â∫è", "‰∫∫ÊÄßÊú¨Ë¥®"],
        "tone": "Âè≤ËØóÊÑü‰∏é‰∏™‰∫∫ÂÜíÈô©Âπ∂Èáç",
        "target_audience": "ÁßëÂπªÁà±Â•ΩËÄÖ"
    }

    context = {
        "user_input": "ÂÜô‰∏Ä‰∏™ÂÖ≥‰∫éÂ§™Á©∫Êé¢Èô©ÁöÑÊïÖ‰∫ã"
    }

    score, breakdown = calculate_narrative_score(data, context, "blueprint")

    print(f"\nScore: {score:.2f} / 30")
    print(f"\nBreakdown:")
    for key, value in breakdown.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")

    return score


def test_characters_scoring():
    """Test scoring a characters task."""
    print("\n" + "=" * 60)
    print("TEST 2: Characters Scoring")
    print("=" * 60)

    data = {
        "characters": [
            {
                "id": "char_001",
                "name": "ÊûóËøúÊòü",
                "archetype": "‰∏çÊÉÖÊÑøÁöÑËã±ÈõÑ",
                "background": "ÂâçÂ∏ùÂõΩÈ£ûË°åÂëòÔºåÂõ†ÊãíÁªùÊâßË°åÂ±†ÊùÄÂπ≥Ê∞ëÁöÑÂëΩ‰ª§ËÄåË¢´ÊµÅÊîæÂà∞ËæπÁºòÊòüÁêÉ„ÄÇ‰ªñÂ§±Âéª‰∫Ü‰∏ÄÂàáÔºåÂè™Ââ©‰∏ã‰∏ÄËâòÁ†¥ÊóßÁöÑË¥ßËàπÂíåÂØπËá™Áî±ÁöÑÊ∏¥Êúõ„ÄÇ",
                "motivation": "‰øùÊä§Êñ∞ÂÆ∂Âõ≠ÁöÑ‰∫∫‰ª¨ÔºåÂêåÊó∂ÊâæÂà∞Ëá™ÊàëÊïëËµéÁöÑÈÅìË∑Ø",
                "skills": ["ÊòüÈôÖÂØºËà™", "ËøëË∫´Ê†ºÊñó", "Êú∫Ê¢∞Áª¥‰øÆ"],
                "personality_traits": ["Ê≤âÈªòÂØ°Ë®Ä", "Ê≠£‰πâÊÑüÂº∫", "ÂÜÖÂøÉÂ≠§Áã¨"]
            },
            {
                "id": "char_002",
                "name": "ËâæÊãâ",
                "archetype": "Á•ûÁßòÂêëÂØº",
                "background": "Â§ñÊòüÈ£ûËàπ‰∏≠ÂîØ‰∏ÄÂπ∏Â≠òÁöÑ‰πòÂÆ¢ÔºåÂ•πÁöÑÁßçÊóèÊã•ÊúâËøúË∂Ö‰∫∫Á±ªÁöÑÊäÄÊúØÔºå‰ΩÜÊ≠£Âú®Ëµ∞ÂêëÁÅ≠‰∫°„ÄÇÂ•πÊòØÊúÄÂêéÁöÑÂ∏åÊúõ„ÄÇ",
                "motivation": "ÂØªÊâæÊñ∞ÁöÑÂÆ∂Âõ≠ÔºåÂêåÊó∂Â∏ÆÂä©‰∫∫Á±ªÈÅøÂÖçÈáçËπàÂ•πÁöÑÁßçÊóèÁöÑË¶ÜËæô",
                "skills": ["ÂøÉÁÅµÊÑüÂ∫î", "È´òÁ∫ßÁßëÊäÄÊìç‰Ωú", "Â§öËØ≠Ë®Ä‰∫§ÊµÅ"],
                "personality_traits": ["Ê∏©Âíå‰ΩÜÂùöÂÆö", "Êô∫ÊÖß", "Áï•Â∏¶ÊÇ≤‰º§"]
            }
        ]
    }

    context = {
        "blueprint": {
            "title": "ÊòüÈôÖËø∑ÈÄî",
            "genre": "ÁßëÂπªÂÜíÈô©"
        }
    }

    score, breakdown = calculate_narrative_score(data, context, "characters")

    print(f"\nScore: {score:.2f} / 30")
    print(f"\nBreakdown:")
    for key, value in breakdown.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")

    return score


def test_chapters_scoring():
    """Test scoring a chapters task."""
    print("\n" + "=" * 60)
    print("TEST 3: Chapters Scoring")
    print("=" * 60)

    data = {
        "chapters": [
            {
                "chapter": 1,
                "title": "Âù†ËêΩÁöÑÊòüËæ∞",
                "content": """
                Â§úÁ©∫‰∏≠ÂàíËøá‰∏ÄÈÅìÁÇΩÁÉàÁöÑÂÖâËäíÔºåÊØî‰ªª‰ΩïÊµÅÊòüÈÉΩË¶ÅÊòé‰∫ÆÁôæÂÄç„ÄÇ

                ÊûóËøúÊòüÊîæ‰∏ãÊâã‰∏≠ÁöÑÊâ≥ÊâãÔºå‰ªéÁ†¥ÊóßË¥ßËàπÁöÑÂºïÊìéËà±‰∏≠Êé¢Âá∫Â§¥Êù•„ÄÇÂú®Ëøô‰∏™Ë¢´Â∏ùÂõΩÈÅóÂøòÁöÑËæπÁºòÊòüÁêÉ‰∏äÔºå‰ªñÂ∑≤Áªè‰π†ÊÉØ‰∫ÜÂØÇÈùôÁöÑÂ§úÊôöÔºå‰ΩÜ‰ªäÊôö‰∏çÂêå„ÄÇ

                ÈÇ£ÈÅìÂÖâËäíÊ≤°ÊúâÊ∂àÂ§±ÔºåÂèçËÄåË∂äÊù•Ë∂äËøë„ÄÇ‰ªñËÉΩÊÑüÂèóÂà∞Á©∫Ê∞î‰∏≠Âº•Êº´ÁöÑËá≠Ê∞ßÂë≥ÔºåËÉΩÂê¨Âà∞ËøúÂ§Ñ‰º†Êù•ÁöÑ‰ΩéÊ≤âËΩ∞È∏£„ÄÇ

                "ÈÇ£‰∏çÊòØÈô®Áü≥Ôºå"‰ªñËá™Ë®ÄËá™ËØ≠ÔºåÁúºÁùõÁ¥ßÁõØÁùÄÈÄêÊ∏êÊ∏ÖÊô∞ÁöÑËΩÆÂªìÔºå"ÈÇ£ÊòØ...‰∏ÄËâòÈ£ûËàπ„ÄÇ"

                È£ûËàπÂù†ËêΩÁöÑÂú∞ÁÇπË∑ùÁ¶ª‰ªñÁöÑÂ∞èÈïá‰∏çÂà∞ÂçÅÂÖ¨Èáå„ÄÇÊûóËøúÊòüÁü•ÈÅìÔºåÊó†ËÆ∫ÈÇ£ÊòØ‰ªÄ‰πàÔºåÂ∏ùÂõΩÁöÑÊé¢ÊµãÂô®ÂæàÂø´Â∞±‰ºöÂèëÁé∞ÂÆÉ„ÄÇ‰ªñÊúâ‰∏§‰∏™ÈÄâÊã©ÔºöÂÅáË£Ö‰ªÄ‰πàÈÉΩÊ≤°ÁúãÂà∞ÔºåÁªßÁª≠Ëøá‰ªñÂπ≥ÈùôÁöÑÊµÅÊîæÁîüÊ¥ªÔºõÊàñËÄÖËµ∂Âú®Â∏ùÂõΩÂÜõÈòüÂà∞Êù•‰πãÂâçÔºåÂéªÁúãÁúãÈÇ£ËâòÈ£ûËàπÈáåÂà∞Â∫ïÊúâ‰ªÄ‰πà„ÄÇ

                ‰ªñÊÉ≥Ëµ∑‰∫Ü‰∫îÂπ¥ÂâçÈÇ£‰∏™ÊîπÂèò‰ªñ‰∏ÄÁîüÁöÑÂ§úÊôöÔºåÊÉ≥Ëµ∑‰∫ÜÈÇ£‰∫õÊó†ËæúÁöÑÈù¢Â≠îÔºåÊÉ≥Ëµ∑‰∫ÜËá™Â∑±‰∏∫‰ªÄ‰πà‰ºöÊù•Âà∞Ëøô‰∏™ËçíÂáâÁöÑÂú∞Êñπ„ÄÇ

                "ËØ•Ê≠ªÁöÑÔºå"‰ªñÊäìËµ∑‰∏ÄÊääÊøÄÂÖâÊâãÊû™ÔºåÊúùÁùÄÂù†ËêΩÁÇπË∑ëÂéªÔºå"ÊàëÂ∞±‰∏çËÉΩÂÆâÂÆâÈùôÈùôÂú∞ÂΩì‰∏™‰øÆÁêÜÂ∑•ÂêóÔºü"
                """,
                "choices": [
                    {
                        "text": "ÊÇÑÊÇÑÊé•ËøëÈ£ûËàπ",
                        "consequence": "ÊûóËøúÊòüÈÄâÊã©‰∫ÜË∞®ÊÖéÁöÑÊñπÂºè",
                        "next_chapter": 2
                    },
                    {
                        "text": "ÂÖàÂè¨ÈõÜÈïá‰∏äÁöÑ‰∫∫",
                        "consequence": "ÊûóËøúÊòüÂÜ≥ÂÆö‰∏çÁã¨Ëá™ÂÜíÈô©",
                        "next_chapter": 3
                    }
                ]
            }
        ]
    }

    context = {
        "blueprint": {
            "title": "ÊòüÈôÖËø∑ÈÄî",
            "genre": "ÁßëÂπªÂÜíÈô©"
        }
    }

    score, breakdown = calculate_narrative_score(data, context, "chapters")

    print(f"\nScore: {score:.2f} / 30")
    print(f"\nBreakdown:")
    for key, value in breakdown.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")

    return score


def test_poor_quality_content():
    """Test scoring poor quality content."""
    print("\n" + "=" * 60)
    print("TEST 4: Poor Quality Content (should score low)")
    print("=" * 60)

    data = {
        "title": "ÊïÖ‰∫ã",
        "genre": "ÁßëÂπª",
        "setting": "Â§™Á©∫",
        "core_conflict": "ÊâìÊû∂",
        "themes": ["Â•Ω"],
        "tone": "‰∏ÄËà¨",
        "target_audience": "‰∫∫"
    }

    context = {
        "user_input": "ÂÜô‰∏Ä‰∏™ÂÖ≥‰∫éÂ§™Á©∫Êé¢Èô©ÁöÑÊïÖ‰∫ã"
    }

    score, breakdown = calculate_narrative_score(data, context, "blueprint")

    print(f"\nScore: {score:.2f} / 30")
    print(f"\nBreakdown:")
    for key, value in breakdown.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")

    return score


def test_evaluator_stats():
    """Test evaluator statistics."""
    print("\n" + "=" * 60)
    print("TEST 5: Evaluator Statistics")
    print("=" * 60)

    evaluator = get_evaluator()
    stats = evaluator.get_stats()

    print(f"\nEvaluator Stats:")
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.2%}" if "rate" in key else f"  {key}: {value:.2f}")
        else:
            print(f"  {key}: {value}")

    return stats


def main():
    print("=" * 60)
    print("StoryNet Narrative Scoring Test Suite")
    print("=" * 60)

    # Check if we have a config
    config_paths = [
        os.path.expanduser("~/.storynet/narrative_config.yaml"),
        os.path.expanduser("~/.storynet/narrative_config.json"),
    ]

    config_found = False
    for path in config_paths:
        if os.path.exists(path):
            print(f"\n‚úÖ Found config at: {path}")
            config_found = True
            break

    if not config_found:
        print("\n‚ö†Ô∏è  No custom config found. Using default settings.")
        print("   For custom evaluation, create: ~/.storynet/narrative_config.yaml")

    # Run tests
    scores = []

    try:
        scores.append(("Blueprint", test_blueprint_scoring()))
    except Exception as e:
        print(f"\n‚ùå Blueprint test failed: {e}")
        scores.append(("Blueprint", None))

    try:
        scores.append(("Characters", test_characters_scoring()))
    except Exception as e:
        print(f"\n‚ùå Characters test failed: {e}")
        scores.append(("Characters", None))

    try:
        scores.append(("Chapters", test_chapters_scoring()))
    except Exception as e:
        print(f"\n‚ùå Chapters test failed: {e}")
        scores.append(("Chapters", None))

    try:
        scores.append(("Poor Quality", test_poor_quality_content()))
    except Exception as e:
        print(f"\n‚ùå Poor quality test failed: {e}")
        scores.append(("Poor Quality", None))

    test_evaluator_stats()

    # Summary
    print("\n" + "=" * 60)
    print("SUMMARY")
    print("=" * 60)

    for name, score in scores:
        if score is not None:
            print(f"  {name}: {score:.2f} / 30")
        else:
            print(f"  {name}: FAILED")

    # Validation
    print("\n" + "=" * 60)
    print("VALIDATION")
    print("=" * 60)

    all_passed = True
    using_fallback = False

    # Check if we're using fallback (no AI available)
    evaluator = get_evaluator()
    stats = evaluator.get_stats()
    if stats["api_errors"] > 0 and stats["cache_hits"] == 0:
        using_fallback = True
        print("‚ö†Ô∏è  AI backend not available - using fallback scores")
        print("   To test with real AI, start Ollama: ollama serve")
        print("")

    # Check that good content scores higher than poor content
    blueprint_score = scores[0][1]
    poor_score = scores[3][1]

    if not using_fallback:
        if blueprint_score is not None and poor_score is not None:
            if blueprint_score > poor_score:
                print("‚úÖ Good content scores higher than poor content")
            else:
                print("‚ùå Good content should score higher than poor content")
                all_passed = False
    else:
        print("‚è≠Ô∏è  Skipping quality comparison (fallback mode)")

    # Check that scores are in valid range
    for name, score in scores:
        if score is not None:
            if 0 <= score <= 30:
                print(f"‚úÖ {name} score in valid range (0-30)")
            else:
                print(f"‚ùå {name} score out of range: {score}")
                all_passed = False

    if all_passed:
        print("\nüéâ All tests passed!")
        if using_fallback:
            print("   (Note: Run with AI backend for full testing)")
    else:
        print("\n‚ö†Ô∏è  Some tests failed. Check the output above.")


if __name__ == "__main__":
    main()
